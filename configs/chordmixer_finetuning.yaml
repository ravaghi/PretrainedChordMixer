defaults:
  - _self_
  - model: chordmixer_finetuning
  - loss: binary_cross_entropy_with_logits
  - dataset: ???
  - dataloader: chordmixer_finetuning
  - trainer: chordmixer
  - wandb: wandb

optimizer:
  _target_: torch.optim.Adam
  lr: 0.00004264

general:
  name: FineTunedChordMixer
  seed: 42
  device_id: 0
  batch_size: 2
  max_epochs: 30
  log_to_wandb: false
  use_multi_gpu: false
  use_scheduler: false
  save_dir: ${hydra:runtime.cwd}/logs/checkpoints/${general.name}/${dataset.type}/${dataset.name}

hydra:
  run:
    dir: ${hydra:runtime.cwd}/logs/hydra/outputs
