program: train.py
method: bayes
metric:
  goal: maximize
  name: test_auc

parameters:
  wandb.name:
    value: ""
  optimizer.lr:
    max: 0.005
    min: 0.000001
    distribution: uniform
  model.decoder_track_size:
    values: [4, 8, 16]
  model.decoder_hidden_size: 
    max: 1000
    min: 50
    q: 50
    distribution: q_uniform
  model.decoder_mlp_dropout: 
    max: 0.5
    min: 0.0
    q: 0.05
    distribution: q_uniform
  model.decoder_layer_dropout: 
    max: 0.5
    min: 0.0
    q: 0.05
    distribution: q_uniform

command:
  - ${env}
  - python
  - ${program}
  - --config-name=chordmixer_finetuning
  - dataset=taxonomy_classification_svb
  - model.model_path=/cluster/home/mahdih/PretrainedChordMixer/models/pcm-vl.pt
  - general.log_to_wandb=true
  - general.batch_size=2
  - ${args_no_hyphens}
